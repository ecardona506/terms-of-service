{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis Semántico Latente\n",
    "En este cuaderno se realizará la implementación del modelo de Analisis Semántico Latente (LSA) para cada uno de los documentos dentro del corpus.\n",
    "\n",
    "Esta etapa esta compuesta de las siguientes fases:\n",
    "1. Cargar datos.\n",
    "2. Tokenizar por frases el corpus.\n",
    "3. Aplicar el preprocesamiento al nuevo corpus.\n",
    "4. Crear diccionarios para relacionar frases e identificadores.\n",
    "5. Construir la matriz de A del Análisis Semántico Latente.\n",
    "6. Aplicar la técnica de Análisis Semántico Latente.\n",
    "7. Generación de resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar elementos necesarios de las librerías\n",
    "import os, shutil, re, pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones auxiliares\n",
    "\n",
    "def tokenize_sentence(path, file_name):\n",
    "    #Retorna un documento tokenizado por frases\n",
    "    doc = []\n",
    "    text = PlaintextCorpusReader(path, file_name)\n",
    "    paragraphs = text.paras()\n",
    "    for paragraph in paragraphs:\n",
    "        for sentence in paragraph:\n",
    "            low, i = 0,0\n",
    "            while i < len(sentence):\n",
    "                token = sentence[i].split('.')\n",
    "                if len(token)-1:\n",
    "                    doc.append(sentence[low:i])\n",
    "                    low=i+1\n",
    "                    i+=2\n",
    "                else:\n",
    "                    i+=1\n",
    "            if low!=i-1:\n",
    "                doc.append(sentence[low:i])\n",
    "    return doc\n",
    "\n",
    "def preprocess(doc, stopwords, stemmer):\n",
    "    #Aplica el preprocesamiento establecido\n",
    "    doc_preprocesed = []\n",
    "    for sentences in doc:\n",
    "        sentence = []\n",
    "        for token in sentences:\n",
    "            if stemmer.stem(token) not in stopwords:\n",
    "                sentence.append(stemmer.stem(token))\n",
    "        if len(sentence) and sentence not in doc_preprocesed:\n",
    "            doc_preprocesed.append(sentence)\n",
    "    return doc_preprocesed\n",
    "\n",
    "def get_dictionaries(doc):\n",
    "    #Retorna un par de diccionarios que relacionan una frase con un id, y un id con una frase.\n",
    "    sentence2id, id2sentence = {},{}\n",
    "    n_sentences = len(doc)\n",
    "    for i in range(n_sentences):\n",
    "        sentence = ' '.join(doc[i])\n",
    "        if sentence not in sentence2id:\n",
    "            sentence2id[sentence] = i\n",
    "            id2sentence[i] = sentence\n",
    "    return sentence2id, id2sentence\n",
    "\n",
    "def getid2token(token2id):\n",
    "    #Retorna un diccionario de tokens a id, a partir de un diccionario de id a tokens\n",
    "    id2token ={}\n",
    "    for k,v in token2id.items():\n",
    "        id2token[v] = k\n",
    "    return id2token\n",
    "\n",
    "def build_A_Matrix(document, tf_idf, token2id, doc_id):\n",
    "    #Construye la matriz A que recibe el modelo de LSA como entrada\n",
    "    data,row_index,col_index = [],[],[]\n",
    "    tf_idf = tf_idf.toarray()\n",
    "    n,m = len(document), len(token2id)\n",
    "    for i in range(n):\n",
    "        sentence = document[i]\n",
    "        j = 0\n",
    "        for token in sentence:\n",
    "            if token in token2id and tf_idf[doc_id,token2id[token]] != 0:\n",
    "                if (j==0) or (j>0 and token2id[token] not in col_index[-j:]):\n",
    "                    value = tf_idf[doc_id,token2id[token]]\n",
    "                    tf_idf_value = value\n",
    "                    data.append(tf_idf_value)\n",
    "                    row_index.append(i)\n",
    "                    col_index.append(token2id[token])\n",
    "                    j+=1\n",
    "    data = np.array(data)\n",
    "    row_index = np.array(row_index)\n",
    "    col_index = np.array(col_index)\n",
    "    A_matrix = csr_matrix((data,(row_index,col_index)),shape=(n, m),dtype=np.float64)\n",
    "    return A_matrix\n",
    "\n",
    "def generate_summary(lsa, n_sentences, sentence2id):\n",
    "    #Genera un resumen con n_sentences frases\n",
    "    total_sentences = lsa.shape[0]\n",
    "    assert n_sentences < total_sentences\n",
    "    columns = [\"topic {}\".format(i) for i in range(n_sentences)]\n",
    "    df = pd.DataFrame(lsa,columns=columns)\n",
    "    df['sentence'] = sentence2id.keys()\n",
    "    summary = []\n",
    "    for i in range(n_sentences):\n",
    "        df = df.sort_values(by='topic {}'.format(i), ascending = False)\n",
    "        j = 0\n",
    "        while j < total_sentences:\n",
    "            sentence = df.iloc[j]['sentence']\n",
    "            if sentence not in summary:\n",
    "                summary.append(sentence)\n",
    "                j=total_sentences\n",
    "            else:\n",
    "                j+=1\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1. Cargar datos.\n",
    "En esta fase se cargarán datos obtenidos durante el preprocesamientoy que utilizaremos para obtener el modelo de LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar el modelo de tf-idf obtenido en preprocesamiento.\n",
    "filename = 'tf-idf_model.pkl'\n",
    "tf_idf = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "#Cargar la matriz de tf-idf obtenido en preprocesamiento.\n",
    "filename = 'tf-idf_matrix.pkl'\n",
    "tf_idf_matrix = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "#Cargar la matriz de tf-idf obtenido en preprocesamiento.\n",
    "filename = 'stopwords.pkl'\n",
    "stopwords = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 2. Tokenizar las frases del corpus.\n",
    "En esta fase se tokenizará por el corpus por frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = \"D:/Documents/Documentos Universidad/Noveno/Proyecto de grado/textos\"\n",
    "stemmer = SpanishStemmer()\n",
    "documento = tokenize_sentence(ruta, 'segurosbolivar-privacidad.txt')\n",
    "#documento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 3. Aplicar el preprocesamiento al nuevo corpus.\n",
    "En esta fase se aplicará el preprocesamiento establecido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "documento = preprocess(documento, stopwords, stemmer)\n",
    "#documento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 4. Crear diccionarios para relacionar frases e identificadores.\n",
    "Con el fin de obtener un resumen luego de aplicar la técnica de LSA, se necesita relacionar una frase con un identificador único. Esta fase cumple con dicho objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase2id, id2frase = get_dictionaries(documento)\n",
    "#id2frase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 5. Construir la matriz de A del Análisis Semántico Latente.\n",
    "Hasta el momento, ya tenemos una lista de frases preprocesadas, ahora lo siguiente que tenemos que hacer es construir una matriz A de NxM, con N frases y M tokens. Para esto nos apoyaremos en el TF-IDF del preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id = tf_idf.vocabulary_\n",
    "matriz_A = build_A_Matrix(documento, tf_idf_matrix, token2id, 2)\n",
    "#matriz_A.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 6. Aplicar la técnica de Análisis Semántico Latente.\n",
    "Una vez construida la matriz A, se utilizará la técnica de LSA para obtener una matriz de MxK, con M frases y K temas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "#Instancia un modelo de LSA\n",
    "lsa_model = TruncatedSVD(n_components=k)\n",
    "\n",
    "#Aplica el LSA a la matriz tf-idf\n",
    "lsa = lsa_model.fit_transform(matriz_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 3)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dimensión de la matriz de LSA\n",
    "lsa.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 7. Generación del resumen\n",
    "A partir del modelo de LSA obtenido en la fase anterior, se genera un resúmen escogiendo una frase por cada tema obtenido dentro del modelo de LSA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[', pon disposicion present avis privac busc inform titul dat personal acerc existent polit tratamient inform sid adopt compañ , form acced caracterist tratamient pretend dar dat',\n",
       " 'conoc , actualiz rectific dat personal frent respons tratamient encarg tratamient',\n",
       " 'compañ identific dat administr , asi activ desarroll dich dat , particul recepcion , conserv , disposicion fin propi contrat desarroll activ complementari refer promocion mercade product servici , asi ofrec compañ hac part grup boliv , pertenec asi indic polit tratamient dispon consult pagin web']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumen = generate_summary(lsa,k,frase2id)\n",
    "resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
