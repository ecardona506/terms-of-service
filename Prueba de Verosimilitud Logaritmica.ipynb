{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba de Verosimilitud Logarítmica\n",
    "\n",
    "En este cuaderno se realizará la implementación de una prueba de verosimilitud logarítmica orientada a generar resúmenes.\n",
    "\n",
    "Para la implementación de esta técnica, se realizarán las siguientes fases:\n",
    "\n",
    "1. Cargar datos.\n",
    "2. Aplicar el preprocesamiento al nuevo corpus.\n",
    "3. Crear diccionarios para relacionar frases e identificadores \n",
    "4. Calcular matriz de verosimilitud logarítmica\n",
    "5. Determinar umbral.\n",
    "6. Construir matriz frases-token con 0 y 1.\n",
    "7. Seleccionar frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar elementos necesarios de las librerías\n",
    "import os, shutil, re, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "from scipy.stats import binom, chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Funciones auxiliares\n",
    "\n",
    "def tokenize_sentence(path, file_name):\n",
    "    #Retorna un documento tokenizado por frases\n",
    "    doc = []\n",
    "    text = PlaintextCorpusReader(path, file_name)\n",
    "    paragraphs = text.paras()\n",
    "    for paragraph in paragraphs:\n",
    "        for sentence in paragraph:\n",
    "            low, i = 0,0\n",
    "            while i < len(sentence):\n",
    "                token = sentence[i].split('.')\n",
    "                if len(token)-1:\n",
    "                    doc.append(sentence[low:i])\n",
    "                    low=i+1\n",
    "                    i+=2\n",
    "                else:\n",
    "                    i+=1\n",
    "            if low!=i-1:\n",
    "                doc.append(sentence[low:i])\n",
    "    return doc\n",
    "\n",
    "def preprocess(doc, stopwords, stemmer):\n",
    "    #Aplica el preprocesamiento establecido\n",
    "    #Adicionalmente, retorna el documento original sin las filas vacías por el preprocesamiento \n",
    "    doc_preprocesed, doc_reduced = [], []\n",
    "    for original_sentence in doc:\n",
    "        preprocessed_sentence = []\n",
    "        for token in original_sentence:\n",
    "            if stemmer.stem(token) not in stopwords:\n",
    "                preprocessed_sentence.append(stemmer.stem(token))\n",
    "        if len(preprocessed_sentence) and preprocessed_sentence not in doc_preprocesed:\n",
    "            doc_preprocesed.append(preprocessed_sentence)\n",
    "            doc_reduced.append(original_sentence)\n",
    "    return doc_preprocesed, doc_reduced\n",
    "\n",
    "def get_dictionaries(doc):\n",
    "    #Retorna un par de diccionarios que relacionan una frase con un id, y un id con una frase.\n",
    "    sentence2id, id2sentence = {},{}\n",
    "    n_sentences = len(doc)\n",
    "    for i in range(n_sentences):\n",
    "        sentence = ' '.join(doc[i])\n",
    "        if sentence not in sentence2id:\n",
    "            sentence2id[sentence] = i\n",
    "            id2sentence[i] = sentence\n",
    "    return sentence2id, id2sentence\n",
    "\n",
    "def log_likelihood_ratio(bag_of_words):\n",
    "    tokens_per_document = bag_of_words.sum(1)\n",
    "    occurrences = bag_of_words.sum(0)\n",
    "    bag_of_words = bag_of_words.toarray()\n",
    "    total_tokens = tokens_per_document.sum(0)[0,0]\n",
    "    n_documents,m_tokens = bag_of_words.shape\n",
    "    loglikelihood_general = []\n",
    "    for i in range(m_tokens):\n",
    "        loglikelihood = binom.logpmf(occurrences[0,i], total_tokens, occurrences[0,i]/total_tokens)\n",
    "        loglikelihood_general.append(loglikelihood)\n",
    "    data = []\n",
    "    for i in range(n_documents):\n",
    "        row = []\n",
    "        tokens_input = tokens_per_document[i,0]\n",
    "        tokens_background = total_tokens - tokens_input\n",
    "        for j in range(m_tokens):\n",
    "            ocurrences_total = occurrences[0,j]\n",
    "            ocurrences_input = bag_of_words[i,j]\n",
    "            ocurrences_background = ocurrences_total - ocurrences_input\n",
    "            loglikelihood_input = binom.logpmf(ocurrences_input, tokens_input, ocurrences_input/tokens_input)\n",
    "            loglikelihood_background = binom.logpmf(ocurrences_background, tokens_background, ocurrences_background/tokens_background)\n",
    "            ratio = loglikelihood_general[j] - loglikelihood_input - loglikelihood_background\n",
    "            row.append(ratio)\n",
    "        data.append(row)\n",
    "    data = np.array(data,dtype=np.float64)\n",
    "    data = data*(-2)\n",
    "    return data\n",
    "\n",
    "def corpus_topic_signatures(log_likelihood_matrix, significance_level):\n",
    "    treshold = chi2.pdf(x=significance_level,df=1)\n",
    "    data, col_index, row_index = [],[],[]\n",
    "    n_documents, m_tokens = log_likelihood_matrix.shape\n",
    "    for i in range(n_documents):\n",
    "        for j in range(m_tokens):\n",
    "            if log_likelihood_matrix[i,j] >= treshold:\n",
    "                col_index.append(i)\n",
    "                row_index.append(j)\n",
    "                data.append(1)\n",
    "    data = np.array(data)\n",
    "    row_index = np.array(row_index)\n",
    "    col_index = np.array(col_index)\n",
    "    topic_signatures = csr_matrix((data,(row_index,col_index)),shape=(n_documents, m_tokens),dtype=np.float64)\n",
    "    return topic_signatures\n",
    "\n",
    "def document_topic_signatures(document, topic_signatures, token2id, doc_id):\n",
    "    #Construye una matriz de un solo documento de acuerdo a la matriz de topic_signatures \n",
    "    data,row_index,col_index = [],[],[]\n",
    "    topic_signatures = topic_signatures.toarray()\n",
    "    n_sentences, m_tokens = len(document), len(token2id)\n",
    "    for i in range(n_sentences):\n",
    "        sentence = document[i]\n",
    "        j = 0\n",
    "        for token in sentence:\n",
    "            if token in token2id and topic_signatures[doc_id,token2id[token]] == 1:\n",
    "                if (j==0) or (j>0 and token2id[token] not in col_index[-j:]):\n",
    "                    value = topic_signatures[doc_id,token2id[token]]\n",
    "                    topic_signature_value = value\n",
    "                    data.append(topic_signature_value)\n",
    "                    row_index.append(i)\n",
    "                    col_index.append(token2id[token])\n",
    "                    j+=1\n",
    "    data = np.array(data)\n",
    "    row_index = np.array(row_index)\n",
    "    col_index = np.array(col_index)\n",
    "    topic_signatures_document = csr_matrix((data,(row_index,col_index)),shape=(n_sentences, m_tokens),dtype=np.float64)\n",
    "    return topic_signatures_document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1. Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar las stopwords obtenido en preprocesamiento.\n",
    "filename = 'stopwords.pkl'\n",
    "stopwords = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "#Carga el corpus usado para el preprocesamiento\n",
    "filename = 'corpus_single_string.pkl'\n",
    "corpus = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 2. Aplicar preprocesamiento\n",
    "### Tokenización por frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = \"D:/Documents/Documentos Universidad/Noveno/Proyecto de grado/textos\"\n",
    "stemmer = SpanishStemmer()\n",
    "bolivar = tokenize_sentence(ruta, 'segurosbolivar-privacidad.txt')\n",
    "mozilla = tokenize_sentence(ruta, 'mozilla-privacidad.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords y Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bolivar_preprocesado, bolivar = preprocess(bolivar, stopwords, stemmer)\n",
    "mozilla_preprocesado, mozilla = preprocess(mozilla, stopwords, stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 3. Crear diccionarios para relacionar frases e identificadores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase2id_bolivar, id2frase_bolivar = get_dictionaries(bolivar)\n",
    "frase2id_mozilla, id2frase_mozilla = get_dictionaries(mozilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 4. Calcular matriz de verosimilitud logarítmica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words = stopwords)\n",
    "bag_of_words = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.04202677e-07,  2.04202677e-07,  1.02092054e-06, ...,\n",
       "         4.08017871e-07, -1.95564343e+00,  6.12382592e-07],\n",
       "       [ 1.39663101e-07,  1.39663101e-07,  6.99770361e-07, ...,\n",
       "         2.79747215e-07,  3.21851883e-06,  4.19324899e-07],\n",
       "       [ 9.55809645e-08,  9.55809645e-08,  4.75879814e-07, ...,\n",
       "         1.90304883e-07, -1.95538247e+00,  2.85105706e-07],\n",
       "       ...,\n",
       "       [ 1.09055254e-07,  1.09055254e-07,  5.43310557e-07, ...,\n",
       "         2.17165884e-07,  2.49577261e-06,  3.25731875e-07],\n",
       "       [ 8.37986094e-08,  8.37986094e-08,  4.17017960e-07, ...,\n",
       "         1.66838626e-07,  1.91412636e-06,  2.49588008e-07],\n",
       "       [ 1.73025893e-07,  1.73025893e-07,  8.63163011e-07, ...,\n",
       "         3.45058735e-07,  3.96779159e-06,  5.17500304e-07]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood = log_likelihood_ratio(bag_of_words)\n",
    "log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 4740)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 5. Determinar umbral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_signatures = corpus_topic_signatures(log_likelihood,0.05)\n",
    "topic_signatures.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 4740)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_signatures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 6. Construir matriz frases-token con 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 4740)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Diccionario que relaciona tokens con un identificador único\n",
    "token2id = vectorizer.vocabulary_\n",
    "\n",
    "#Lista de textos dentro del corpus\n",
    "textos = os.listdir(ruta)\n",
    "\n",
    "topic_signatures = document_topic_signatures(bolivar_preprocesado, topic_signatures, token2id, textos.index('segurosbolivar-privacidad.txt'))\n",
    "topic_signatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
