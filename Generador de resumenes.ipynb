{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generador de resumenes\n",
    "\n",
    "Este cuaderno tiene como próposito generar un resumen de un texto de términos y condiciones utilizando el modelo más óptimo discutido en este proyecto. Para esto, necesitamos de un texto en formato txt, el cual será modificado utilizando la variable nombre_texto para el nombre del texto, con el sufijo .txt . Este texto deberá estar en la misma ubicación que este cuaderno. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar elementos necesarios de las librerías\n",
    "import os, shutil, re, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import setrecursionlimit\n",
    "from rouge import Rouge\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "from scipy.stats import binom\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "#Aumentar la el limite de la recursión\n",
    "setrecursionlimit(10**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_texto = os.getcwd()\n",
    "nombre_texto = \"waze-demo.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones auxiliares\n",
    "\n",
    "def tokenize_and_stemm(path,file_name):\n",
    "    #Tokeniza un texto y aplica el stemming de snowball\n",
    "    file = []\n",
    "    stemmer = SpanishStemmer()\n",
    "    text = PlaintextCorpusReader(path, file_name) #Cambiar ruta por '/'\n",
    "    tokens = text.words()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "def tokenize_sentence(path, file_name):\n",
    "    #Retorna un documento tokenizado por frases\n",
    "    doc = []\n",
    "    text = PlaintextCorpusReader(path, file_name)\n",
    "    paragraphs = text.paras()\n",
    "    for paragraph in paragraphs:\n",
    "        for sentence in paragraph:\n",
    "            low, i = 0,0\n",
    "            while i < len(sentence):\n",
    "                token = sentence[i].split('.')\n",
    "                if len(token)-1:\n",
    "                    doc.append(sentence[low:i])\n",
    "                    low=i+1\n",
    "                    i+=2\n",
    "                else:\n",
    "                    i+=1\n",
    "            if low!=i-1:\n",
    "                doc.append(sentence[low:i])\n",
    "    return doc\n",
    "\n",
    "def preprocess(doc, stopwords, stemmer):\n",
    "    #Aplica el preprocesamiento establecido\n",
    "    #Adicionalmente, retorna el documento original sin las filas vacías por el preprocesamiento \n",
    "    doc_preprocesed, doc_reduced = [], []\n",
    "    for original_sentence in doc:\n",
    "        preprocessed_sentence = []\n",
    "        for token in original_sentence:\n",
    "            if stemmer.stem(token) not in stopwords:\n",
    "                preprocessed_sentence.append(stemmer.stem(token))\n",
    "        if len(preprocessed_sentence) and preprocessed_sentence not in doc_preprocesed:\n",
    "            doc_preprocesed.append(preprocessed_sentence)\n",
    "            doc_reduced.append(original_sentence)\n",
    "    return doc_preprocesed, doc_reduced\n",
    "\n",
    "def get_dictionaries(doc):\n",
    "    #Retorna un par de diccionarios que relacionan una frase con un id, y un id con una frase.\n",
    "    sentence2id, id2sentence = {},{}\n",
    "    n_sentences = len(doc)\n",
    "    for i in range(n_sentences):\n",
    "        sentence = ' '.join(doc[i])\n",
    "        if sentence not in sentence2id:\n",
    "            sentence2id[sentence] = i\n",
    "            id2sentence[i] = sentence\n",
    "    return sentence2id, id2sentence\n",
    "\n",
    "def reverse_dict(dictionary):\n",
    "    #Invierte el sentido de un diccionario\n",
    "    reverse = {}\n",
    "    for k,v in dictionary.items():\n",
    "        reverse[v] = k\n",
    "    return reverse\n",
    "\n",
    "def log_likelihood_ratio(bag_of_words):\n",
    "    #Calcula la matriz -2 lambda\n",
    "    tokens_per_document = bag_of_words.sum(1)\n",
    "    ocurrences = bag_of_words.sum(0)\n",
    "    total_tokens = bag_of_words.sum(0).sum(1)[0,0]\n",
    "    n_documents,m_tokens = bag_of_words.shape\n",
    "    loglikelihood_general = []\n",
    "    for i in range(m_tokens):\n",
    "        ocurrences_general = ocurrences[0,i]\n",
    "        loglikelihood = binom.pmf(ocurrences_general, total_tokens,ocurrences_general/total_tokens)\n",
    "        loglikelihood_general.append(loglikelihood)\n",
    "    data = []\n",
    "    for i in range(n_documents):\n",
    "        row = []\n",
    "        tokens_input = tokens_per_document[i,0]\n",
    "        tokens_background = total_tokens - tokens_input\n",
    "        for j in range(m_tokens):\n",
    "            ocurrences_total = ocurrences[0,j]\n",
    "            ocurrences_input = bag_of_words[i,j]\n",
    "            ocurrences_background = ocurrences_total - ocurrences_input\n",
    "            loglikelihood_input = binom.pmf(ocurrences_input, tokens_input, ocurrences_input/tokens_input)\n",
    "            loglikelihood_background = binom.pmf(ocurrences_background, tokens_background, ocurrences_background/tokens_background)\n",
    "            ratio = loglikelihood_general[j]/(loglikelihood_input*loglikelihood_background)\n",
    "            row.append(ratio)\n",
    "        data.append(row)\n",
    "    lambda_matrix = np.array(data,dtype=np.float64)\n",
    "    return lambda_matrix\n",
    "\n",
    "def corpus_topic_signatures(log_likelihood_matrix, treshold):\n",
    "    #Crea una matriz con los topic signatures a partir de la matriz -2lambda y el umbral\n",
    "    data, col_index, row_index = [],[],[]\n",
    "    n_documents, m_tokens = log_likelihood_matrix.shape\n",
    "    for i in range(n_documents):\n",
    "        for j in range(m_tokens):\n",
    "            if log_likelihood_matrix[i,j] >= treshold:\n",
    "                row_index.append(i)\n",
    "                col_index.append(j)\n",
    "                data.append(1)\n",
    "    data = np.array(data)\n",
    "    row_index = np.array(row_index)\n",
    "    col_index = np.array(col_index)\n",
    "    topic_signatures = csr_matrix((data,(row_index,col_index)),shape=(n_documents, m_tokens),dtype=np.float64)\n",
    "    return topic_signatures\n",
    "\n",
    "def document_topic_signatures(document, topic_signatures, token2id, doc_id):\n",
    "    #Construye una matriz de un solo documento de acuerdo a la matriz de topic signatures \n",
    "    data,row_index,col_index = [],[],[]\n",
    "    topic_signatures = topic_signatures.toarray()\n",
    "    n_sentences, m_tokens = len(document), len(token2id)\n",
    "    for i in range(n_sentences):\n",
    "        sentence = document[i]\n",
    "        j = 0\n",
    "        for token in sentence:\n",
    "            if token in token2id and topic_signatures[doc_id,token2id[token]] == 1:\n",
    "                if (j==0) or (j>0 and token2id[token] not in col_index[-j:]):\n",
    "                    topic_signature_value = topic_signatures[doc_id,token2id[token]]\n",
    "                    data.append(topic_signature_value)\n",
    "                    row_index.append(i)\n",
    "                    col_index.append(token2id[token])\n",
    "                    j+=1\n",
    "    data = np.array(data)\n",
    "    row_index = np.array(row_index)\n",
    "    col_index = np.array(col_index)\n",
    "    topic_signatures_document = csr_matrix((data,(row_index,col_index)),shape=(n_sentences, m_tokens),dtype=np.float64)\n",
    "    return topic_signatures_document\n",
    "\n",
    "def generate_summary(topic_signatures, n_sentences, sentence2id):\n",
    "    #Genera un resumen con un total de n_sentences frases\n",
    "    total_sentences = topic_signatures.shape[0]\n",
    "    assert n_sentences < total_sentences\n",
    "    topic_signatures = topic_signatures.toarray().sum(1)\n",
    "    df = pd.DataFrame(topic_signatures,columns = ['topic signatures'])\n",
    "    df['sentence'] = sentence2id.keys()\n",
    "    df = df.sort_values(by='topic signatures', ascending = False)\n",
    "    summary = []\n",
    "    i,flag=0,1\n",
    "    while i < n_sentences and flag:\n",
    "        sentence = df.iloc[i]['sentence']\n",
    "        flag = df.iloc[i]['topic signatures']\n",
    "        if flag:\n",
    "            summary.append(sentence)\n",
    "        i+=1\n",
    "    assert summary != []\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Waze puede recoger y registrar con qué frecuencia y durante cuánto tiempo utilizas la Aplicación ; qué tipo de dispositivo tienes ; tus identificadores únicos ; el tipo y la versión del sistema operativo ; el uso de la batería ; las páginas web o aplicaciones de terceros que visitas o utilizas , o con las que interactúas a través de la Aplicación ; la información que has consultado en nuestros Servicios ; los anuncios que te muestra , los que ves o en los que haces clic ; el hecho de que hayas usado los Servicios para comunicarte con otros usuarios y el hecho de que hayas usado servicios o aplicaciones de terceros a través de Waze para comunicarte con terceros , y la duración de dichas comunicaciones ; la dirección de protocolo de Internet ( IP ) y el nombre del dominio con el que accedes a los Servicios ; y la ubicación geográfica del dispositivo que usas para iniciar sesión en Waze y utilizar la Aplicación o el Sitio Web. Waze vinculará toda tu información con tu Cuenta y con ese nombre de usuario , incluida la información que reciba de todos los dispositivos móviles que ejecuten la Aplicación en los que hayas iniciado sesión con tu nombre de usuario y contraseña ; A excepción de lo que se describe en el punto siguiente , y tal como se explica más adelante en la sección “ Visibilidad ”, tu Cuenta y la información asociada a ella solo se mostrarán a Waze ; y Cuando utilices la Aplicación , tu ubicación aproximada y cualquier otra información que decidas proporcionar se mostrarán a los demás usuarios asociados a ese nombre de usuario. Waze puede usar cookies con distintas finalidades como , por ejemplo : evitar que tengas que volver a introducir tu nombre de usuario y contraseña cada vez que inicies sesión , facilitar el uso del Sitio Web , recoger datos estadísticos , verificar información , personalizar el Sitio Web según tus preferencias personales y proteger tu información. Datos que quieres compartir con Waze como , por ejemplo : tu nombre de usuario ; los informes asociados a este nombre de usuario ; números de teléfono ; ubicaciones de casa , del trabajo o de otros lugares favoritos ; destinos a los que puedes ir ; consultas de búsqueda ; información del calendario ( si activas y utilizas determinadas funciones del Servicio , como los recordatorios de hora de salida ) y los archivos que has subido al Servicio , incluidos los de voz y audio , si procede. Cuentas de usuario y acceso a los Servicios Información que recoge Waze Por qué recoge Waze estos datos Compartir tu información Visibilidad Integración en redes sociales Integración en otros servicios de terceros Controlar tu información personal Cómo retiene Waze los datos que recoge Cómo usa Waze la información agregada Cookies Campañas publicitarias Seguridad de la información Cumplimiento y cooperación con autoridades reguladoras Cambios en esta Política de Privacidad. Waze puede usar información anónima , estadística o agregada ( incluidos datos de ubicación anónimos ), de modo que resulte imposible identificar a los usuarios , para prestar los Servicios correctamente , mejorar su calidad , optimizar tu experiencia , crear servicios y funciones ( incluidos servicios personalizados ), modificar o cancelar contenido o servicios , y para otras finalidades internas , comerciales y estadísticas ( como la publicidad. A excepción de lo que se describe en el punto siguiente , y tal como se explica más adelante en la sección “ Visibilidad ”, tu Cuenta y la información asociada a ella ( incluido tu identificador único ) solo se mostrarán a Waze ; y Cuando utilices la Aplicación , tu ubicación aproximada y cualquier otra información que decidas proporcionar se mostrarán a los demás usuarios asociados al nombre de usuario general “ Wazer. Para ayudar a los anunciantes a decidir qué anuncios deben mostrar en Waze , en Google , y en los sitios web y las aplicaciones asociados con Google , Waze puede compartir información con estos servicios , como identificadores únicos de publicidad ( IDs de publicidad de los dispositivos móviles Android o tecnologías similares. Waze trabaja con partners publicitarios , incluidas algunas entidades que forman parte del grupo de empresas de Google , para poder mostrarte anuncios más útiles y relevantes en función de tu actividad en los Servicios y de la información asociada al identificador de tu dispositivo ( identificadores de publicidad o tecnologías similares ) que le facilitan estos partners publicitarios. En cualquier caso ( exceptuando lo que se indica más abajo en relación con la información que se comparte desde tus cuentas de redes sociales según la configuración de privacidad que hayas definido en dichas cuentas ), tu dirección de correo electrónico no se mostrará a otros usuarios en ningún informe ni en ninguna publicación de usuario que compartas o subas'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cargar las stopwords obtenido en preprocesamiento.\n",
    "filename = 'stopwords.pkl'\n",
    "stopwords = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "#Carga el corpus usado para el preprocesamiento\n",
    "filename = 'corpus_single_string.pkl'\n",
    "corpus_single_string = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "#Tokenización y stemming en un solo string\n",
    "texto = tokenize_and_stemm(ruta_texto, nombre_texto)\n",
    "\n",
    "#if texto not in corpus_single_string: \n",
    "corpus_single_string.append(texto)\n",
    "    \n",
    "#Tokenización por frases para los resumenes\n",
    "texto_frases = tokenize_sentence(ruta_texto, nombre_texto)\n",
    "stemmer = SpanishStemmer()\n",
    "texto_frases_con_stemm, texto_frases = preprocess(texto_frases, stopwords, stemmer)\n",
    "\n",
    "#Diccionarios\n",
    "frase2id, id2frase = get_dictionaries(texto_frases)\n",
    "\n",
    "#Matriz de frecuencia o bag of words\n",
    "vectorizer = CountVectorizer(stop_words = stopwords)\n",
    "\n",
    "bag_of_words = vectorizer.fit_transform(corpus_single_string)\n",
    "\n",
    "#Diccionario que relaciona tokens con un identificador único\n",
    "token2id = vectorizer.vocabulary_\n",
    "\n",
    "#Diccionario que un identificador con un token\n",
    "id2token = reverse_dict(token2id)\n",
    "\n",
    "#Matriz de verosimilitud logarítmica (lambda)\n",
    "lambda_matrix = log_likelihood_ratio(bag_of_words)\n",
    "\n",
    "#Detectar topic signatures de todo el corpus\n",
    "umbral = 5.0239\n",
    "topic_signatures = corpus_topic_signatures(lambda_matrix,umbral)\n",
    "\n",
    "#Aplicar topic signatures al texto\n",
    "indice_texto = -1\n",
    "topic_signatures = document_topic_signatures(texto_frases_con_stemm, topic_signatures, token2id, indice_texto)\n",
    "\n",
    "#Generar el resumen\n",
    "n_frases = 10\n",
    "resumen = generate_summary(topic_signatures, n_frases, frase2id)\n",
    "resumen = '. '.join(resumen)\n",
    "resumen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
